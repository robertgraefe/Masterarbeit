{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix Decompositions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determinant and Trace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A determinant is a mathematical object in the analysis and solution of systems of linear equations and are only defined for square matrices $A \\in R^{nxn}$. It is a function that maps a matrix $A$ onto a real number.\n",
    "\n",
    "$det(A) = |A|$\n",
    "\n",
    "A determinant can be interpreted as the area in $R^{2x2}$ and as the volume in $R^{3x3}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing for Invertablility\n",
    "\n",
    "$A \\in R^{1x1} \\to A$ is a scalar $a$.\n",
    "\n",
    "$A=a \\to A^{-1}=\\frac{1}{a}\\:if\\:a \\neq 0$\n",
    "\n",
    "$A \\in R^{2x2}$\n",
    "\n",
    "$AA^{-1} = I \\to A^{-1} = \\frac{1}{a_{11}a_{22}-a_{12}a_{21}} \\begin{bmatrix} a_{22} & -a_{12} \\\\ -a_{21} & a_{11} \\end{bmatrix}\\:if\\:a_{11}a_{22}-a_{12}a_{21} \\neq 0$\n",
    "\n",
    "$det(A) = \\begin{vmatrix} a_{11} & a_{12} \\\\ a_{21} & a_{22} \\end{vmatrix} = a_{11}a_{22}-a_{12}a_{21}$\n",
    "\n",
    "If $det \\neq 0$ than the matrix is invertable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determinant of triangular matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A square matrix $T$ is called an $\\textbf{upper_triangular matrix}$ if it contains only 0 below its diagonal.\n",
    "\n",
    "$\\begin{bmatrix}\n",
    "3 & 2 & 3 & 4 \\\\\n",
    "0 & 5 & 5 & 6 \\\\\n",
    "0 & 0 & 0 & 7 \\\\\n",
    "0 & 0 & 0 & 9 \\\\\n",
    "\\end{bmatrix}$\n",
    "\n",
    "A square matrix $T$ is called an $\\textbf{lower_triangular matrix}$ if it contains only 0 above its diagonal.\n",
    "\n",
    "$\\begin{bmatrix}\n",
    "3 & 0 & 0 & 0 \\\\\n",
    "2 & 5 & 0 & 0 \\\\\n",
    "3 & 5 & 3 & 0 \\\\\n",
    "4 & 2 & 5 & 9 \\\\\n",
    "\\end{bmatrix}$\n",
    "\n",
    "For a triangular matrix the determinant is the product of the diagonal elements.\n",
    "\n",
    "$det(T) = \\prod_{i=1}^{n}{T_{ii}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determinant in $R^{nxn}$ with the recursive Laplace Expansion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a matrix $A \\in R^{nxn}$.\n",
    "\n",
    "- Expansion along column $j$:\n",
    "$det(A) = \\sum_{k=1}^{n} (-1)^{k+j}a_{kj}det(A_{k,j})$\n",
    "\n",
    "- Expansion along row $j$:\n",
    "$det(A) = \\sum_{k=1}^{n} (-1)^{k+j}a_{jk}det(A_{j,k})$\n",
    "\n",
    "$A_{k,j} \\in R^{n-1xn-1}$ is a submatrix of A that is obtained by deleting row k and column j."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "$A = \\begin{bmatrix}\n",
    "1&2&3\\\\\n",
    "3&1&2\\\\\n",
    "0&0&1\\\\\n",
    "\\end{bmatrix}$\n",
    "\n",
    "Applying the expansion along the first row:\n",
    "\n",
    "$\\begin{vmatrix}\n",
    "1&2&3\\\\\n",
    "3&1&2\\\\\n",
    "0&0&1\\\\\n",
    "\\end{vmatrix}\n",
    "= (-1)^{1+1} \\cdot 1 \\begin{vmatrix} 1&2\\\\0&1 \\end{vmatrix}\n",
    "+ (-1)^{1+2} \\cdot 2 \\begin{vmatrix} 3&2\\\\0&1 \\end{vmatrix}\n",
    "+ (-1)^{1+3} \\cdot 3 \\begin{vmatrix} 3&1\\\\0&0 \\end{vmatrix}\n",
    "$\n",
    "\n",
    "$\n",
    "= 1 \\cdot 1(1-0)+(-1) \\cdot 2(3-0)+ 1 \\cdot 3(0-0) = -5\n",
    "$\n",
    "\n",
    "$det(A) = -5$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determinant Properties\n",
    "\n",
    "- $det(AB) = det(A) det(B)$\n",
    "- $det(A) = det(A^T)$\n",
    "- if $A$ is regular invertible then $det(A^{-1}) =  \\frac{1}{det(A)}$\n",
    "- similar matrices possess the same determinant\n",
    "- adding a multiple of a row/column to another one does not change $det(A)$\n",
    "- scalar multiplication of A: $det(\\lambda A) = \\lambda^n det(A)$\n",
    "- swapping two rows/columns changes the sign of $det(A)$\n",
    "\n",
    "Those properties allow for the use of Gaussian elimination to compute $det(A)$:\n",
    "- bring $A$ into row-echelon form\n",
    "- stop if $A$ is a triangular matrix\n",
    "- calculate $det(A)$\n",
    "\n",
    "A square matrix $A \\in R^{nxn}$ has $det(A) \\neq 0$ if $rk(A)=n$. $\\to A$ is invertible if it is full rank.  \n",
    "\n",
    "The $\\textbf{trace}$ of a square matrix $A \\in R^{nxn}$ is defined as:\n",
    "\n",
    "$\n",
    "tr(A) := \\sum_{i=1}^{n} a_{ii}\n",
    "\\to\n",
    "$ the trace is the sum of the diagonal elements.\n",
    "\n",
    "- $tr(A+B) = tr(A)+tr(B)$\n",
    "- $tr(\\alpha A) = \\alpha tr(A)$\n",
    "- $tr(I_n) = n$\n",
    "- $tr(AB) = tr(BA)$\n",
    "- $tr(xy^T)=tr(y^Tx)=y^Tx$\n",
    "- $tr(B)=tr(S^{-1}AS)=tr(ASS^{-1})=tr(A) \\to$ matrix representations of linar mappings are basis dependent; the trace of a linar mapping is basis independent \n",
    "\n",
    "The $\\textbf{Characteristic Polynomial}$ is defined as:\n",
    "\n",
    "$p_A(\\lambda):=det(A- \\lambda I) = c_0+c_1 \\lambda + c_2 \\lambda ^2+ ... + c_{n-1} \\lambda^{n-1} + (-1)^n \\lambda^n$\n",
    "\n",
    "where $c_0, ...c_{n-1} \\in R$ is the  $\\textbf{characteristic polynomial}$ of $A$.\n",
    "\n",
    "- $c_0=det(A)$\n",
    "- $c_{n-1}=(-1)^{n-1}tr(A)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eigenvalues & Eigenvectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $A \\in R^{nxn}$ is a square matric than $\\lambda \\in R$ is an $\\textbf{eigenvalue}$ and $x \\in R^n \\backslash \\{0\\}$ is an $\\textbf{eigenvector}$ of $A$ if \n",
    "\n",
    "$Ax = \\lambda x$\n",
    "\n",
    "$\\lambda \\in R$ in an eigenvalue of $A$ if $\\lambda$ is a root of the characteristic polynominal $p_A(\\lambda)$ of $A$.\n",
    "\n",
    "The writing $\\lambda_i$ refers to the algebraic multiplicity and describes the number of times the root appears in the characteristic polynominal.\n",
    "\n",
    "The set of all eigenvectors of $A$ with eigenvalue $\\lambda $ spans a subspace which is called $\\textbf{eigenspace}$ of $A$ and is denoted by $E_{\\lambda}$.\n",
    "The eigenspace is the solution space of the homogeneous system of linear equations $(A- \\lambda I)x=0$.\n",
    "\n",
    "The set of all eigenvalues of $A$ is called $\\textbf{eigenspectrum}$ or just spectrum of $A$.\n",
    "\n",
    "If $\\lambda_i$ is a eigenvalue of $A$ then the geometric multiplicity of $\\lambda_i$ is the number of linearly independent eigenvectors associated with $\\lambda_i$. In other words, it is the dimensionality of the\n",
    "eigenspace spanned by the eigenvectors associated with $\\lambda_i$.\n",
    "\n",
    "The eigenvalues of $A \\in R^{nxn}$ with $n$ distinct eigenvalues $\\lambda_1,...,\\lambda_2$ are linearly independent. This states that eigenvectors of a matrix with $n$ distinct eigenvalues form a basis of $R^n$.\n",
    "\n",
    "A square matrix is $\\textbf{defective}$ if it possesses fewer than $n$ linearly independent eigenvectors.\n",
    "\n",
    "Given a matrix $A \\in R^{mxn}$ we can always obtain a symetric, positive semidefinite matrix $S \\in R^{nxn}$ by defining $S := A^TA$. If $rk(A)=n$ then $S$ is symetric, positive definite.\n",
    "\n",
    "If $A \\in R^{nxn}$ is symmetric, there exists  an orthogonal basis of the corresponding vector space $V$ consisting of eigenvectors of $A$, and each eigenvalue is real.\n",
    "\n",
    "The determinant of a matrix $A \\in R^{nxn}$ is the product of its eigenvalues: $det(A)=\\prod_{i=1}^{n}\\lambda_i$.\n",
    "\n",
    "The trace of a matrix $A \\in R^{nxn}$ is the sum of its eigenvalues: $tr(A)=\\sum_{i=1}^{n} \\lambda_i$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collinearity and Codirection\n",
    "\n",
    "Two vectors are codirected if they point in the same direction.\n",
    "Two vectors are collinear if they point in the same or opposite direction.\n",
    "\n",
    "For any $c \\in R \\backslash \\{0\\}$ it holds that $cx$ is an eigenvector of A with the same eigenvalue. Any collinear vectors to $x$ are eigenvectors of $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example\n",
    "\n",
    "$A= \\begin{bmatrix} 4&2 \\\\ 1&3 \\end{bmatrix}$\n",
    "\n",
    "##### Eigenvalues\n",
    "\n",
    "$p_a(\\lambda) = det(A-\\lambda I) = det(\\begin{bmatrix} 4&2 \\\\ 1&3 \\end{bmatrix} - \\begin{bmatrix} \\lambda&0 \\\\ 0&\\lambda \\end{bmatrix}) = det(\\begin{bmatrix} 4-\\lambda&2 \\\\ 1&3-\\lambda \\end{bmatrix}) = (4-\\lambda)(3-\\lambda)-2 \\cdot 1 = 10-7\\lambda+\\lambda^2 = (2-\\lambda)(5-\\lambda)$\n",
    "\n",
    "roots: $\\lambda_1 = 2, \\lambda_2 = 5$\n",
    "\n",
    "##### Eigenvectors and Eigenspaces\n",
    "\n",
    "$ \\begin{bmatrix} 4-\\lambda&2 \\\\ 1&3-\\lambda \\end{bmatrix}x=0$\n",
    "\n",
    "for $\\lambda = 5$ we obtain:\n",
    "\n",
    "$\\begin{bmatrix} 4-5&2 \\\\ 1&3-5 \\end{bmatrix}\\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} = \\begin{bmatrix} -1&2 \\\\ 1&-2 \\end{bmatrix}\\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} = 0$\n",
    "\n",
    "by solving this homogeneous system we obtain a solution space:\n",
    "\n",
    "$E_5=span(\\begin{bmatrix}2 \\\\1\\end{bmatrix}) \\to$ one dimensional eigenspace\n",
    "\n",
    "for $\\lambda = 2$ we obtain:\n",
    "\n",
    "$\\begin{bmatrix} 4-2&2 \\\\ 1&3-2 \\end{bmatrix}\\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} = \\begin{bmatrix} 2&2 \\\\ 1&1 \\end{bmatrix}\\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} = 0$\n",
    "\n",
    "$E_5=span(\\begin{bmatrix}2 \\\\1\\end{bmatrix}) \\to$ one dimensional eigenspace\n",
    "\n",
    "$E_2=span(\\begin{bmatrix}1 \\\\-1\\end{bmatrix}) $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cholesky Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can decompose numbers in different ways. An example is given by the square-root operation which decomposes a number into a factor of two identical components: $9=3\\cdot3$.\n",
    "The Cholesky Decomposition is the equivalent decomposition for symmetric, positive definite matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A symmetric, positive definite matrix $A$ can be factorized into a product $A=LL^T$, where $L$ is a lower-triangular matrix with positive diagonal elements. $L$ is called the Cholesky factor of $A$ and $L$ is unique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example\n",
    "\n",
    "Let $A \\in R^3$ be a symmetric, positive definite matrix.\n",
    "\n",
    "$A=LL^T$\n",
    "\n",
    "$\n",
    "\\begin{bmatrix} \n",
    "a_{11}&a_{21}&a_{31}\\\\\n",
    "a_{12}&a_{22}&a_{32}\\\\\n",
    "a_{13}&a_{23}&a_{33}\\\\\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix} \n",
    "l_{11}&0&0\\\\\n",
    "l_{12}&l_{22}&0\\\\\n",
    "l_{13}&l_{23}&a_{33}\\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix} \n",
    "l_{11}&l_{21}&l_{31}\\\\\n",
    "0&l_{22}&l_{32}\\\\\n",
    "0&0&l_{33}\\\\\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "$\n",
    "A = \n",
    "\\begin{bmatrix} \n",
    "l_{11}^2&l_{21}l_{11}&l_{31}l_{11}\\\\\n",
    "l_{21}l_{11}&l_{21}^2+l_{22}^2&l_{31}l_{21}+l_{32}l_{22}\\\\\n",
    "l_{31}l_{11}&l_{31}l_{32}+l_{32}l_{22}&l_{31}^2+l_{32}^2+l_{33}^2\\\\\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "$\n",
    "l_{11}=\\sqrt{a_{11}}\n",
    "$\n",
    "\n",
    "$\n",
    "l_{22}=\\sqrt{a_{22}-l_{21}^2}\n",
    "$\n",
    "\n",
    "$\n",
    "l_{33}=\\sqrt{a_{33}-(l_{31}^2+l_{32}^2)}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Eigendecomposition and Diagonalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A $\\textbf{diagonal matrix}$ is a matrix that has zeros on all elements except the diagonal. Diagonal matrices allow for fast computation of determinants, powers and inverses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A matrix $A \\in R^{nxn}$ is $\\textbf{diagonalizable}$ if it is similar to a diagonal matrix, i.e., if there exists an invertible matrix $P \\in R^{nxn}$ such that $D=P^{-1}AP$.\n",
    "\n",
    "Diagonal matrices $D$ can efficiently be raised to a power.\n",
    "\n",
    "Assume that the eigendecomposition $A = P DP ^{−1}$ exists. Then $det(A)=det(PDP^{-1})=det(P)det(D)det(P^-1)= det(D)=\\prod d_{ii}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eigendecomposition\n",
    "\n",
    "A square matrix $A \\in R^{nxn}$ can be factored into $A=PDP^{-1}$, where $P \\in R^{nxn}$ and $D$ is a diagonal matrix  whose diagonal entries are the eigenvalues of $A$  if and only if the eigenvectors of $A$ form a basis of $R^n$.\n",
    "\n",
    "A symmetric matrix $S \\in R^{nxn}$ can always be diagonalized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example Eigendecomposition\n",
    "\n",
    "$A=\\frac12 \\begin{bmatrix}\n",
    "5&-2\\\\\n",
    "-2&5\n",
    "\\end{bmatrix}$\n",
    "\n",
    "Step 1: Compute eingenvalues and eigenvectors\n",
    "\n",
    "eigenvalues:\n",
    "\n",
    "$det(\\begin{bmatrix}\n",
    "\\frac52-\\lambda&-2\\\\\n",
    "-2&\\frac52-\\lambda\n",
    "\\end{bmatrix})=(\\frac52-\\lambda^2)-1=(\\lambda-\\frac72)(\\lambda-\\frac32) \\to$ $\\lambda_1=\\frac72$, $\\lambda_2=\\frac32$\n",
    "\n",
    "eigenvectors:\n",
    "\n",
    "$\\begin{bmatrix}\n",
    "2&1\\\\\n",
    "1&2\n",
    "\\end{bmatrix} p_1=\\frac72p_1 \\to p_1=\\frac{1}{\\sqrt2}\\begin{bmatrix}1\\\\-1\\end{bmatrix}$\n",
    "\n",
    "$\\begin{bmatrix}\n",
    "2&1\\\\\n",
    "1&2\n",
    "\\end{bmatrix} p_2=\\frac32p_2 \\to p_2=\\frac{1}{\\sqrt2}\\begin{bmatrix}1\\\\1\\end{bmatrix}$\n",
    "\n",
    "Setp 2: Check for existance\n",
    "\n",
    "The eigenvectors $p_1$, $p_2$ form a basis of $R^2$. Therefore, $A$ can be diagonalized.\n",
    "\n",
    "Step 3: Construct the matrix $P$ to diagonalize $A$. We collect the eigenvectors of $A$ in $P$ so that:\n",
    "\n",
    "$P=\\begin{bmatrix}p_1&P-2\\end{bmatrix}= \\frac{1}{\\sqrt2}\\begin{bmatrix}1&1\\\\-1&1\\end{bmatrix}$\n",
    "\n",
    "We obtain:\n",
    "\n",
    "$P^{-1}AP = \\begin{bmatrix}\\frac72&0\\\\0&\\frac32\\end{bmatrix}=D$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Singular Value Decomposition (SVD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A SVD of a matrix $A$ represents a linear mapping $\\phi$ that quantifies the change between the underlying geometry of those twoo vector spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$A$ is a rectangular matrix of rank $r \\in [0,min(m,n)]$\n",
    "\n",
    "The SVD of $A$ is a decomposition of the form:\n",
    "\n",
    "$A_{mxn}=U_{mxm} \\sum_{mxn} V^T_{nxn}$\n",
    "\n",
    "- $U \\in R^{mxm}$ is an orthogonal matrix with column vectors $i = 1,...,m$ (left-singular vectors)\n",
    "- $V \\in R^{nxn}$ is an orthogonal matrix with column vectors $i = 1,...,n$ (right-singular vectors)\n",
    "- $\\sum \\in R^{mxn}$ is a matrix with $\\sum_{ii}=\\sigma_i \\geq0$ and $\\sum_{ij}=0, i\\neq j$\n",
    "\n",
    "The diagonal entries of $\\sum$ are called the singular values.\n",
    "\n",
    "$\\sum$ is unique and rectangular. It has the same size as $A$, which means that $\\sum$ has a diagonal submatrix that contains the singular values and needs additional zero padding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construction of the SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SVD is the eigendecomposition of a SPD matrix: \n",
    "\n",
    "$S=S^T=PDP^T$\n",
    "\n",
    "$S=U \\sum V^T$ for $U=P=V$ and $D=\\sum$\n",
    "\n",
    "Computing the SVD of $A$ is equivalent to finding two sets of orthonogal bases $U$ and $V$ of the codomains $R^m$ and $R^n$.\n",
    "\n",
    "#### right-singular vectors\n",
    "\n",
    "$A^TA \\in R^{nxn}=V \\begin{bmatrix} \\sigma_1^2 & 0 & 0 \\\\ 0 & ... & 0 \\\\ 0 & 0 & \\sigma_{n}^2 \\end{bmatrix} V^T$ \n",
    "\n",
    "The eigenvectors of $A^TA$ that compose $P$ are the right-singular vectors $V$ of $A$. The eigenvalues of $A^TA$ are the squared singular values of $\\sum$.\n",
    "\n",
    "#### left-singular vectors\n",
    "\n",
    "$AA^T \\in R^{mxm}=U \\begin{bmatrix} \\sigma_1^2 & 0 & 0 \\\\ 0 & ... & 0 \\\\ 0 & 0 & \\sigma_{m}^2 \\end{bmatrix} U^T$ \n",
    "\n",
    "#### singular values\n",
    "\n",
    "$u_i := \\frac{Av_i}{\\lVert Av_i \\rVert}=\\frac{1}{\\sqrt{\\lambda_i}}Av_i=\\frac{1}{\\sigma_i}Av_i \\to Av_i=\\sigma_i u_i$\n",
    "\n",
    "$AV=U\\sum$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example\n",
    "\n",
    "$\n",
    "A=\\begin{bmatrix}\n",
    "1&0&1\\\\\n",
    "-2&1&0\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "##### right-singular vectors\n",
    "\n",
    "$A^TA=\n",
    "\\begin{bmatrix}\n",
    "1&-2\\\\\n",
    "0&1\\\\\n",
    "1&0\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "1&0&1\\\\\n",
    "-2&1&0\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "5&-2&1\\\\\n",
    "-2&1&0\\\\\n",
    "1&0&1\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "$\\to$ eigenvalue decomposition\n",
    "\n",
    "\n",
    "$\n",
    "A^TA=\n",
    "\\begin{bmatrix}\n",
    "\\frac{5}{\\sqrt{30}}&0&\\frac{-1}{\\sqrt{6}}\\\\\n",
    "\\frac{-2}{\\sqrt{30}}&\\frac{1}{\\sqrt{5}}&\\frac{-2}{\\sqrt{6}}\\\\\n",
    "\\frac{1}{\\sqrt{30}}&\\frac{2}{\\sqrt{5}}&\\frac{1}{\\sqrt{6}}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "6&0&0\\\\\n",
    "0&1&0\\\\\n",
    "0&0&0\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\frac{5}{\\sqrt{30}}&\\frac{-2}{\\sqrt{30}}&\\frac{1}{\\sqrt{30}}\\\\\n",
    "\\frac{-2}{\\sqrt{30}}&\\frac{1}{\\sqrt{5}}&\\frac{2}{\\sqrt{5}}\\\\\n",
    "\\frac{-1}{\\sqrt{6}}&\\frac{-2}{\\sqrt{6}}&\\frac{1}{\\sqrt{6}}\n",
    "\\end{bmatrix}\n",
    "=PDP^T\n",
    "$\n",
    "\n",
    "$V=P=\\begin{bmatrix}\n",
    "\\frac{5}{\\sqrt{30}}&0&\\frac{-1}{\\sqrt{6}}\\\\\n",
    "\\frac{-2}{\\sqrt{30}}&\\frac{1}{\\sqrt{5}}&\\frac{-2}{\\sqrt{6}}\\\\\n",
    "\\frac{1}{\\sqrt{30}}&\\frac{2}{\\sqrt{5}}&\\frac{1}{\\sqrt{6}}\n",
    "\\end{bmatrix}$\n",
    "\n",
    "##### singular-value matrix\n",
    "\n",
    "The singular values $\\sigma$ are the square roots of the eigenvalues of $A^TA$ which can be obtain by $D$.\n",
    "Since $rk(A)=2$, there are only two nonzero singular values: $\\sigma_1=\\sqrt6, \\sigma_2=\\sqrt1=1$\n",
    "Since the singular value matrix must be the same size as $A$ we obtain:\n",
    "$\\sum=\\begin{bmatrix} \\sqrt6 & 0 & 0 \\\\ 0 & 1 & 0 \\end{bmatrix}$\n",
    "\n",
    "##### left-singular vectors as the normalized image of the right-singular vectors\n",
    "\n",
    "$u_1 = \\frac{1}{\\sigma_1}Av_1=\\frac{1}{\\sqrt6}\\begin{bmatrix}\n",
    "1&0&1\\\\\n",
    "-2&1&0\n",
    "\\end{bmatrix}\\begin{bmatrix} \\frac{5}{\\sqrt{30}} \\\\ \\frac{-2}{\\sqrt{30}} \\\\ \\frac{1}{\\sqrt{30}}\\end{bmatrix}\n",
    "= \\begin{bmatrix} \\frac{1}{\\sqrt5} \\\\ -\\frac{2}{\\sqrt5}\\end{bmatrix}$\n",
    "\n",
    "$u_2 = \\frac{1}{\\sigma_2}Av_2=\\frac{1}{1}\\begin{bmatrix}\n",
    "1&0&1\\\\\n",
    "-2&1&0\n",
    "\\end{bmatrix}\\begin{bmatrix} 0 \\\\ \\frac{1}{\\sqrt{5}} \\\\ \\frac{2}{\\sqrt{5}}\\end{bmatrix}\n",
    "= \\begin{bmatrix} \\frac{2}{\\sqrt5} \\\\ \\frac{1}{\\sqrt5}\\end{bmatrix}$\n",
    "\n",
    "$U = \\begin{bmatrix} u_1 & u_2 \\end{bmatrix} = \\frac{1}{\\sqrt5} \\begin{bmatrix} 1 & 2 \\\\ -2 & 1 \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eigenvalue Decomposition vs Singular Value Decomposition\n",
    "\n",
    "- SVD always exists for any matrix $R \\in R{^mxn}$\n",
    "- eigendecomposition is only defined for squared matrices $\\in R^{nxn}$\n",
    "- the vectors in the eigendecomposition matrix $P$ are not necessarily orthogonal\n",
    "- the vectorsin $U$ and $V$ in SVD are orthonormal and represent rotations\n",
    "- $V$ and $U$ in SVD are generally not inverse of each other\n",
    "- $P$ and $P^{-1}$ are inverses of each other\n",
    " for symmetric decompositions the SVD and eigendecomposition are the same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we investigate how the SVD allows for the representation of $A$ as the sum of simpler (low-rank) matrices $A_i$.\n",
    "\n",
    "$A_i := u_iv_i^T \\to$ outer product of the ith orthogonal column vector of $U$ and $V$.\n",
    "\n",
    "A matrix $A \\in R^mxn$ of rank $r$ can be written as the sum of rank 1-matrices $A_i$ so that:\n",
    "\n",
    "$A=\\sum_{i=1}^{r} \\sigma_i u_i v_i^T = \\sum_{i=1}^{r} \\sigma_i A_i$\n",
    "\n",
    "If we do not sum up all matrices to get $A$ but only up to an intermediate value $k<r$ we obtain a rank-k approximation.\n",
    "\n",
    "$\\hat A(k)=\\sum_{i=1}^{k} \\sigma_i u_i v_i^T = \\sum_{i=1}^{k} \\sigma_i A_i$\n",
    "\n",
    "Spectral Norm:\n",
    "\n",
    "for $x \\in R^n\\backslash \\{0\\}$ the spectral norm of a matrix $A \\in R^{mxn}$ is defined as:\n",
    "\n",
    "$\\lVert A \\rVert _2 := max \\frac{\\lVert Ax  \\rVert_2}{\\lVert x \\rVert_2}$\n",
    "\n",
    "The spectral norm of $A$ is its largest singular value $\\sigma_1$.\n",
    "\n",
    "Eckhard-Young Theorem:\n",
    "\n",
    "$A \\in R^{mxn}$ of rank $r$ and $B \\in R^{mxn}$ of rank $k$.\n",
    "\n",
    "For any $k \\leq r$ with $\\hat A (k) = \\sum_{i=1}^k \\sigma_i u_i v_i^T$ it holds that:\n",
    "\n",
    "$\\hat A (k) = argmin_{rk(B)=k} \\lvert A-B \\rVert _2$\n",
    "\n",
    "$\\lvert A- \\hat A(k) \\rVert _2 = \\sigma_{k+1}$\n",
    "\n",
    "The Eckart-Young theorem states explicitly how much error we introduce by approximating $A$ using a rank-k approximation. The rank k-approximation can be interpreted as a projection of the full-rank matrix $A$ onto a lower-dimensional space of rank-at-most-k matrices.\n",
    "Of all possible projections the SVD minimizes the error between $A$ and any rank-k approximation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
